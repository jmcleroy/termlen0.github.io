<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Network Automation</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2016-12-30T05:52:09-06:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Ajay Chenampara</name>
   <email></email>
 </author>

 
 <entry>
   <title>The need for a stateful variable tracker and an implementation example</title>
   <link href="http://localhost:4000/2016/12/28/observations/"/>
   <updated>2016-12-28T00:00:00-06:00</updated>
   <id>http://localhost:4000/2016/12/28/observations</id>
   <content type="html">&lt;p&gt;When it comes to automation role models, network engineers have often looked up,
to our compute brethren. For decades, compute admins have had tools that allowed
them to execute scripts on systems at particular times: typically backups,
rsync etc. More recently, in the VM universe,  DevOps tools like Chef/Puppet/
Ansible, have empowered &amp;#39;Developer Administrators&amp;#39;, to stand up the entire app
stack, automatically.&lt;/p&gt;

&lt;p&gt;In my network automation journey, I realized early on that, a big gap/obstacle
for network automation is the need for a backend, to track positive
integers; a source of truth, that knows what numeric value was last
assigned to a particular network &lt;em&gt;service&lt;/em&gt;(viz. firewall contexts,
vlans etc  for a 3 tier app in the DMZ. Traditional networks are
relatively static(from a configuration
standpoint). We typically make incremental changes to &lt;em&gt;variables&lt;/em&gt; in a production
network configuration. For instance, once a port-channel
is created, say Po101, we typically have some internal standard as to how the
next port-channel will be numbered (could be Po102, Po201 etc). For a given
&amp;#39;network service&amp;#39;, we have to track many similar variables: Vlan numbers, Vrf numbers,
HSRP group numbers, subinterface numbers, AS numbers.... The list goes on.
Now, in quick contrast, the compute folks have almost never had this problem.
Most automation in that space is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Stand up the VM&lt;/li&gt;
&lt;li&gt;Install necessary middleware&lt;/li&gt;
&lt;li&gt;Clone the app repo&lt;/li&gt;
&lt;li&gt;Fire up the app&lt;/li&gt;
&lt;li&gt;Ensure compliance&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;What about IP addresses:&lt;/h2&gt;

&lt;p&gt;Obviously, I am oversimplifying a bit, but the point remains that, there really
isn&amp;#39;t much state tracking, when it comes to application/OS admin automation.
Back in the day (actually, less than 10 yrs ago), I remember when
compute/application admins were very fond of static IP addresses.
That used to be &amp;#39;stateful&amp;#39; variable they needed tracked. Not any more.
Unfortunately, on the network side, we are still very dependent (depending on the use
case) on static IP addressing for our devices. Needless to say a solid IPAM
is an extremely important stateful variable tracker for network
automation. The reason for this blog post is, however, to address the
&amp;quot;other&amp;quot; variables (vlan numbers, vif numbers et al), we need for
network automation, that doesn&amp;#39;t really come built into standard,
&amp;#39;network focused&amp;#39; software, like IPAMs/CMDB&lt;/p&gt;

&lt;h2&gt;An implementation example using NSoT:&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/dropbox/nsot&quot;&gt;NSoT&lt;/a&gt; is an opensource
IPAM(primarily) from the folks at dropbox. Last year at the NetDevOps
workshop at Interop,  &lt;a href=&quot;https://twitter.com/jathanism&quot;&gt;Jathan&lt;/a&gt; demo&amp;#39;ed
the product. It had 2 things that caught my attention:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It was written with an API first approach&lt;/li&gt;
&lt;li&gt;It was written in python (a language that I am least uncomfortable
in :) )&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I forked the repo and implemented the &amp;quot;Iterables&amp;quot; API, with a lot of
guidance and support from Jathan. We have since, internally, used my
implementation of NSoT, to prove out quite a few automation scenarios.&lt;/p&gt;

&lt;h2&gt;Iterables - A visualization:&lt;/h2&gt;

&lt;p&gt;My implementation of the stateful variables, involves 2 tables. One
table tracks the variable that needs to be iterated (vlans numbers,
vif numbers etc, along with the increment value). The other table
tracks all allocated values for a given variable.
My good friend &lt;a href=&quot;https://www.linkedin.com/in/bobbyoutlaw&quot;&gt;Bobby Outlaw&lt;/a&gt;
helped visualize the idea as follows:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/iterables.png&quot;&gt;&lt;img src=&quot;/assets/iterables.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Using NSoT as a backend, the above drawing visualizes various network
automation services, within a service provider. &lt;em&gt;Service A&lt;/em&gt; requires the
network automation system to keep track of a cryptomap sequence
number, tenant VRF number and a portchannel interface
number. Alternatively &lt;em&gt;Service B&lt;/em&gt; requires the automation system to
track the cryptomap sequence number and the tenant vrf number.
Each time we invoke the &amp;quot;playbook&amp;quot; for service A, we can now make API
calls to our NSoT backend, that will give us the next available unused
integer for the given variable.
This particular implementation, also gives you an unique &lt;em&gt;&amp;quot;Service
Key&amp;quot;&lt;/em&gt; each time you invoke the playbook. The idea behind it is; as a
service provider, you hand over the unique key to your customer as a
reference to the service request. If the customer wants the service
revoked or modified (pretty much any CRUD operation)  at a later date,
the service key can be used to identify all the values associated with
a particular service, and thus modified.&lt;/p&gt;

&lt;h2&gt;Demo Playbook:&lt;/h2&gt;

&lt;p&gt;If you&amp;#39;d like to play around with the implementation, you can download
a copy &lt;a href=&quot;https://github.com/termlen0/nsot&quot;&gt;here&lt;/a&gt;. Please keep in mind
that you will have to follow
the
&lt;a href=&quot;https://nsot.readthedocs.io/en/latest/development.html&quot;&gt;developer guide&lt;/a&gt;
to compile from source. (Hopefully once the PR is approved, it should
be available for general consumption, directly from pip). Once you have NSoT running, you
should be able to grab
the &lt;a href=&quot;https://github.com/termlen0/nsot-tester&quot;&gt;iterable test playbook&lt;/a&gt;
to get an idea of how to use the stateful backend for your automation
playbooks.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Running ansible at scale</title>
   <link href="http://localhost:4000/2016/12/16/observations/"/>
   <updated>2016-12-16T00:00:00-06:00</updated>
   <id>http://localhost:4000/2016/12/16/observations</id>
   <content type="html">&lt;p&gt;Last week I deployed my first &amp;quot;at scale&amp;quot; playbook. The overall objective was simple: Add new dhcp helper address to about 400 switches. 
Like most things though, the devil is in the details. Right off the bat, I ran into &amp;quot;non-ansible&amp;quot; related issues (tacacs/ssh).
That brought the number of devices to about 270. Not a big number right? At the most basic level, yes, if I was simply pushing configs using the ios_config
module.&lt;/p&gt;

&lt;h2&gt;Breakdown of the playbook&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Execute a show run on the device and compile a local backup for each device&lt;/li&gt;
&lt;li&gt;Run a pre-flight report, specific to the interfaces we are going to impact (multiple ssh sessions per host)&lt;/li&gt;
&lt;li&gt;Build the configs locally&lt;/li&gt;
&lt;li&gt;Deploy the configs&lt;/li&gt;
&lt;li&gt;Validate the configs/Unit testing&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;A bit more about the unit testing:&lt;/h3&gt;

&lt;p&gt;For testing the changes were deployed, I had 2 criteria:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Assert that the new helpers are present within the interface configurations (of the specific interfaces)&lt;/li&gt;
&lt;li&gt;Assert that the startup and running config are in sync (in other word, the new config has been saved)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For assertion 1, I took the approach of running a show running interface per interface - this implied multiple ssh sessions per host.&lt;/p&gt;

&lt;h2&gt;Observations:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Running the playbook for backups result in a lot of SSH connection failures on the first run. Subsequent runs are significantly more successful - Still see some failures&lt;/li&gt;
&lt;li&gt;Running the playbook for the preflight report/validation, results in ssh timeouts - these are not consistent across hosts: Meaning, for the same host, the show run int
for Vlan101 will work but might fail for Vlan201 on the first run, but on the next run, there is no guarantee that a repeat play will reproduce this exact failure &lt;/li&gt;
&lt;li&gt;My validation role uses dynamic includes like this &lt;a href=&quot;https://github.com/termlen0/ansible_dynamic_include_bug_demo&quot;&gt;example&lt;/a&gt;. Running the playbook with a tag other
than &amp;quot;validate&amp;quot;, still attempts to load all yaml files and results in failure.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Tweaks and next steps:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;I had mixed success with using the &amp;quot;serial&amp;quot; &lt;a href=&quot;http://docs.ansible.com/ansible/playbooks_delegation.html#rolling-update-batch-size&quot;&gt;option&lt;/a&gt;. Needs more trial and error. &lt;/li&gt;
&lt;li&gt;Rewrite pre-flight and validation scripts to only do a single show run. Collect specific interface details from a local copy&lt;/li&gt;
&lt;li&gt;Tried pipelining and some other recommendations that seemed relevant based on &lt;a href=&quot;https://www.ansible.com/blog/ansible-performance-tuning&quot;&gt;this&lt;/a&gt;. 
However, it appears to be focussed on using ssh connections to the remote systems. As we know, for the ios_* modules, ansible ssh&amp;#39;es to the local host and then uses paramiko
within the modules. In short, pipeline did not seem to do much &lt;/li&gt;
&lt;li&gt;Setting the timeout parameter for the ios_* modules seemed to have no affect on the ssh timeouts. &lt;/li&gt;
&lt;li&gt;To further understand observation 1 (which is still the most vexing one), I tried connecting to the device using paramiko from the python interpreter and executing 
the same commands. I could not recreate the issue. I had good connectivity each time&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Other issues:&lt;/h2&gt;

&lt;p&gt;For observation 3, I opened an &lt;a href=&quot;https://github.com/ansible/ansible/issues/19345&quot;&gt;issue&lt;/a&gt; with ansible. Based on the comments it was closed with, I guess, it is an expected
behavior. Which means, for any playbook that has dynamic includes, we need to remember to send any variables that role will need, even though your tags may not be calling
that role.&lt;/p&gt;
</content>
 </entry>
 

</feed>
